{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxsAnSB50qVt"
      },
      "source": [
        "Clone the Github repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NYvrQ0ygHmt",
        "outputId": "1cb8f317-139b-45fe-e454-411174e418c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KDD2022CLARE'...\n",
            "remote: Enumerating objects: 381, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 381 (delta 58), reused 99 (delta 37), pack-reused 236\u001b[K\n",
            "Receiving objects: 100% (381/381), 67.90 MiB | 22.14 MiB/s, done.\n",
            "Resolving deltas: 100% (166/166), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/vDarfur2/KDD2022CLARE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ySW_wQYgw82",
        "outputId": "18864951-a976-4367-b0e4-ee67049066d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KDD2022CLARE\n"
          ]
        }
      ],
      "source": [
        "%cd KDD2022CLARE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5b4bgab1q-I"
      },
      "source": [
        "Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxMi6CV9hZ2a",
        "outputId": "3813e49d-f045-4e5c-8d6b-30ef17a2348b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.4.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2023.11.17)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX\n",
        "!pip install torch\n",
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCDde5ii01D2"
      },
      "source": [
        "Youtube Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-gzb0pbcJ9P",
        "outputId": "b2246db8-3532-4983-dd31-68459c823fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= = = = = = = = = = = = = = = = = = = = \n",
            "##  Starting Time: 2023-12-12 23:37:27\n",
            "Namespace(seed=0, device='cuda:0', dataset='youtube', num_pred=1000, num_train=90, num_val=10, gnn_type='GCN', n_layers=2, hidden_dim=64, output_dim=64, margin=0.6, comm_max_size=12, locator_lr=0.001, locator_epoch=30, locator_batch_size=256, agent_lr=0.001, n_episode=10, n_epoch=1000, gamma=0.99, max_step=10, max_rewrite_step=4, commr_path='', writer_dir='ckpts/youtube/20231212-233727')\n",
            "[YOUTUBE] #Nodes 93013, #Edges 182713, #Communities 1000\n",
            "Finish loading data: Data(x=[93013, 5], edge_index=[2, 365426])\n",
            "\n",
            "Split dataset: #Train 90, #Val 10, #Test 900\n",
            "\n",
            "Community Locator init ... \n",
            "GNNEncoder(\n",
            "  (act): LeakyReLU(negative_slope=0.01)\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): GCNConv(5, 64)\n",
            "    (1): GCNConv(64, 64)\n",
            "  )\n",
            ")\n",
            "Community Locator finish initialization!\n",
            "\n",
            "Training Order Embedding ... \n",
            "***epoch: 0001 | ORDER EMBEDDING train_loss: 619.75500 | cost time 3.72s\n",
            "***epoch: 0002 | ORDER EMBEDDING train_loss: 563.31982 | cost time 2.22s\n",
            "***epoch: 0003 | ORDER EMBEDDING train_loss: 691.05609 | cost time 2.05s\n",
            "***epoch: 0004 | ORDER EMBEDDING train_loss: 413.49915 | cost time 2.1s\n",
            "***epoch: 0005 | ORDER EMBEDDING train_loss: 559.72632 | cost time 2.04s\n",
            "***epoch: 0006 | ORDER EMBEDDING train_loss: 488.79791 | cost time 2.2s\n",
            "***epoch: 0007 | ORDER EMBEDDING train_loss: 546.02655 | cost time 2.71s\n",
            "***epoch: 0008 | ORDER EMBEDDING train_loss: 512.95435 | cost time 2.17s\n",
            "***epoch: 0009 | ORDER EMBEDDING train_loss: 636.18054 | cost time 2.06s\n",
            "***epoch: 0010 | ORDER EMBEDDING train_loss: 695.18518 | cost time 1.98s\n",
            "***epoch: 0011 | ORDER EMBEDDING train_loss: 526.23560 | cost time 1.97s\n",
            "***epoch: 0012 | ORDER EMBEDDING train_loss: 547.15790 | cost time 1.99s\n",
            "***epoch: 0013 | ORDER EMBEDDING train_loss: 453.53467 | cost time 2.94s\n",
            "***epoch: 0014 | ORDER EMBEDDING train_loss: 458.72522 | cost time 2.3s\n",
            "***epoch: 0015 | ORDER EMBEDDING train_loss: 405.39636 | cost time 2.06s\n",
            "***epoch: 0016 | ORDER EMBEDDING train_loss: 364.70068 | cost time 2.0s\n",
            "***epoch: 0017 | ORDER EMBEDDING train_loss: 510.41626 | cost time 1.99s\n",
            "***epoch: 0018 | ORDER EMBEDDING train_loss: 477.17755 | cost time 1.93s\n",
            "***epoch: 0019 | ORDER EMBEDDING train_loss: 338.40167 | cost time 2.46s\n",
            "***epoch: 0020 | ORDER EMBEDDING train_loss: 471.03619 | cost time 2.37s\n",
            "***epoch: 0021 | ORDER EMBEDDING train_loss: 413.11301 | cost time 1.97s\n",
            "***epoch: 0022 | ORDER EMBEDDING train_loss: 377.98883 | cost time 2.01s\n",
            "***epoch: 0023 | ORDER EMBEDDING train_loss: 384.48032 | cost time 2.08s\n",
            "***epoch: 0024 | ORDER EMBEDDING train_loss: 368.49200 | cost time 2.04s\n",
            "***epoch: 0025 | ORDER EMBEDDING train_loss: 370.39899 | cost time 2.35s\n",
            "***epoch: 0026 | ORDER EMBEDDING train_loss: 402.45511 | cost time 2.5s\n",
            "***epoch: 0027 | ORDER EMBEDDING train_loss: 335.91266 | cost time 2.0s\n",
            "***epoch: 0028 | ORDER EMBEDDING train_loss: 281.41638 | cost time 1.97s\n",
            "***epoch: 0029 | ORDER EMBEDDING train_loss: 271.54419 | cost time 1.98s\n",
            "***epoch: 0030 | ORDER EMBEDDING train_loss: 286.09753 | cost time 2.09s\n",
            "Order Embedding Finish Training!\n",
            "\n",
            "***Generate nodes embedding from idx 0 to 4096\n",
            "***Generate nodes embedding from idx 4096 to 8192\n",
            "***Generate nodes embedding from idx 8192 to 12288\n",
            "***Generate nodes embedding from idx 12288 to 16384\n",
            "***Generate nodes embedding from idx 16384 to 20480\n",
            "***Generate nodes embedding from idx 20480 to 24576\n",
            "***Generate nodes embedding from idx 24576 to 28672\n",
            "***Generate nodes embedding from idx 28672 to 32768\n",
            "***Generate nodes embedding from idx 32768 to 36864\n",
            "***Generate nodes embedding from idx 36864 to 40960\n",
            "***Generate nodes embedding from idx 40960 to 45056\n",
            "***Generate nodes embedding from idx 45056 to 49152\n",
            "***Generate nodes embedding from idx 49152 to 53248\n",
            "***Generate nodes embedding from idx 53248 to 57344\n",
            "***Generate nodes embedding from idx 57344 to 61440\n",
            "***Generate nodes embedding from idx 61440 to 65536\n",
            "***Generate nodes embedding from idx 65536 to 69632\n",
            "***Generate nodes embedding from idx 69632 to 73728\n",
            "***Generate nodes embedding from idx 73728 to 77824\n",
            "***Generate nodes embedding from idx 77824 to 81920\n",
            "***Generate nodes embedding from idx 81920 to 86016\n",
            "***Generate nodes embedding from idx 86016 to 90112\n",
            "***Generate nodes embedding from idx 90112 to 93013\n",
            "\n",
            "Start Matching ... \n",
            "[Generate] Pred size 1000, Avg Length 8.0460\n",
            "\n",
            "P, R, F, J AvgAxis0:  [0.30234517 0.61807817 0.3728929  0.25334436]\n",
            "P, R, F, J AvgAxis1:  [0.47906782 0.2335766  0.28884251 0.19884008]\n",
            "AvgF1: 0.3309 AvgJaccard: 0.2261 NMI: 0.0953 Detect percent: 0.5058\n",
            "[Eval-Epoch100] Improve f1 -0.0234, improve jaccard -0.0184, improve new_nmi 0.0351\n",
            "[Eval-Epoch160] Improve f1 0.0199, improve jaccard 0.0165, improve new_nmi 0.0846\n",
            "[Eval-Epoch180] Improve f1 0.0079, improve jaccard 0.0080, improve new_nmi 0.0041\n",
            "[Eval-Epoch200] Improve f1 0.0250, improve jaccard 0.0213, improve new_nmi 0.0496\n",
            "[Eval-Epoch220] Improve f1 0.0299, improve jaccard 0.0263, improve new_nmi 0.0495\n",
            "[Eval-Epoch240] Improve f1 0.0217, improve jaccard 0.0171, improve new_nmi 0.0456\n",
            "[Eval-Epoch260] Improve f1 0.0150, improve jaccard 0.0148, improve new_nmi 0.0503\n",
            "[Eval-Epoch280] Improve f1 0.0201, improve jaccard 0.0162, improve new_nmi 0.0038\n",
            "[Eval-Epoch300] Improve f1 0.0497, improve jaccard 0.0408, improve new_nmi 0.0940\n",
            "[Eval-Epoch320] Improve f1 0.0411, improve jaccard 0.0346, improve new_nmi 0.0943\n",
            "[Eval-Epoch340] Improve f1 0.0356, improve jaccard 0.0303, improve new_nmi 0.1182\n",
            "[Eval-Epoch360] Improve f1 0.0490, improve jaccard 0.0415, improve new_nmi 0.0958\n",
            "[Eval-Epoch380] Improve f1 0.0370, improve jaccard 0.0326, improve new_nmi 0.0952\n",
            "[Eval-Epoch400] Improve f1 0.0398, improve jaccard 0.0390, improve new_nmi 0.0588\n",
            "[Eval-Epoch420] Improve f1 0.0352, improve jaccard 0.0327, improve new_nmi 0.0557\n",
            "[Eval-Epoch440] Improve f1 0.0213, improve jaccard 0.0205, improve new_nmi 0.0094\n",
            "[Eval-Epoch460] Improve f1 0.0287, improve jaccard 0.0229, improve new_nmi 0.0037\n",
            "[Eval-Epoch480] Improve f1 0.0280, improve jaccard 0.0222, improve new_nmi 0.0039\n",
            "[Eval-Epoch500] Improve f1 0.0247, improve jaccard 0.0205, improve new_nmi 0.0041\n",
            "[Eval-Epoch520] Improve f1 0.0398, improve jaccard 0.0332, improve new_nmi 0.0895\n",
            "[Eval-Epoch540] Improve f1 0.0234, improve jaccard 0.0194, improve new_nmi 0.0039\n",
            "[Eval-Epoch560] Improve f1 0.0321, improve jaccard 0.0243, improve new_nmi -0.0067\n",
            "[Eval-Epoch580] Improve f1 0.0447, improve jaccard 0.0368, improve new_nmi 0.0497\n",
            "[Eval-Epoch600] Improve f1 0.0494, improve jaccard 0.0466, improve new_nmi 0.0620\n",
            "[Eval-Epoch620] Improve f1 0.0238, improve jaccard 0.0193, improve new_nmi 0.0040\n",
            "[Eval-Epoch640] Improve f1 0.0492, improve jaccard 0.0468, improve new_nmi 0.1006\n",
            "[Eval-Epoch660] Improve f1 0.0409, improve jaccard 0.0344, improve new_nmi 0.0420\n",
            "[Eval-Epoch680] Improve f1 0.0270, improve jaccard 0.0227, improve new_nmi 0.0486\n",
            "[Eval-Epoch700] Improve f1 0.0331, improve jaccard 0.0239, improve new_nmi 0.0343\n",
            "[Eval-Epoch720] Improve f1 0.0470, improve jaccard 0.0402, improve new_nmi 0.0536\n",
            "[Eval-Epoch740] Improve f1 0.0380, improve jaccard 0.0335, improve new_nmi 0.0491\n",
            "[Eval-Epoch760] Improve f1 0.0442, improve jaccard 0.0379, improve new_nmi 0.0516\n",
            "[Eval-Epoch780] Improve f1 0.0253, improve jaccard 0.0209, improve new_nmi 0.0441\n",
            "[Eval-Epoch800] Improve f1 0.0208, improve jaccard 0.0161, improve new_nmi -0.0067\n",
            "[Eval-Epoch820] Improve f1 0.0304, improve jaccard 0.0244, improve new_nmi 0.0437\n",
            "[Eval-Epoch840] Improve f1 0.0097, improve jaccard 0.0100, improve new_nmi 0.0040\n",
            "[Eval-Epoch860] Improve f1 0.0246, improve jaccard 0.0201, improve new_nmi 0.0040\n",
            "[Eval-Epoch880] Improve f1 0.0270, improve jaccard 0.0227, improve new_nmi 0.0468\n",
            "[Eval-Epoch900] Improve f1 0.0361, improve jaccard 0.0297, improve new_nmi 0.0498\n",
            "[Eval-Epoch920] Improve f1 0.0328, improve jaccard 0.0272, improve new_nmi 0.0467\n",
            "[Eval-Epoch940] Improve f1 0.0460, improve jaccard 0.0380, improve new_nmi 0.0499\n",
            "[Eval-Epoch960] Improve f1 0.0518, improve jaccard 0.0427, improve new_nmi 0.0943\n",
            "[Eval-Epoch980] Improve f1 0.0409, improve jaccard 0.0342, improve new_nmi 0.0940\n",
            "[Eval-Epoch1000] Improve f1 0.0244, improve jaccard 0.0230, improve new_nmi 0.0095\n",
            "Load net from ckpts/youtube/20231212-233727/commr_eval_best.pt at Epoch959\n",
            "[Rewrite] Pred size 997, Avg Length 6.7021\n",
            "P, R, F, J AvgAxis0:  [0.33737429 0.57331996 0.38434552 0.26515076]\n",
            "P, R, F, J AvgAxis1:  [0.44574459 0.26353487 0.2963643  0.20737974]\n",
            "AvgF1: 0.3404 AvgJaccard: 0.2363 NMI: 0.1159 Detect percent: 0.4731\n",
            "## Finishing Time: 2023-12-12 23:58:12\n",
            "= = = = = = = = = = = = = = = = = = = = \n"
          ]
        }
      ],
      "source": [
        "!python main.py --dataset=youtube"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FpiDtGB1DJl"
      },
      "source": [
        "Orkut Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI0ADjBoVByr",
        "outputId": "d16aaf20-2547-4b12-b2bd-3bb210b99564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "= = = = = = = = = = = = = = = = = = = = \n",
            "##  Starting Time: 2023-12-11 21:53:44\n",
            "Namespace(seed=0, device='cuda:0', dataset='orkut', num_pred=1000, num_train=90, num_val=10, gnn_type='GCN', n_layers=2, hidden_dim=64, output_dim=64, margin=0.6, comm_max_size=12, locator_lr=0.001, locator_epoch=30, locator_batch_size=256, agent_lr=0.001, n_episode=10, n_epoch=1000, gamma=0.99, max_step=10, max_rewrite_step=4, commr_path='', writer_dir='ckpts/orkut/20231211-215344')\n",
            "[ORKUT] #Nodes 11369, #Edges 16367, #Communities 1000\n",
            "Finish loading data: Data(x=[11369, 5], edge_index=[2, 32734])\n",
            "\n",
            "Split dataset: #Train 90, #Val 10, #Test 900\n",
            "\n",
            "Community Locator init ... \n",
            "GNNEncoder(\n",
            "  (act): LeakyReLU(negative_slope=0.01)\n",
            "  (conv_layers): ModuleList(\n",
            "    (0): GCNConv(5, 64)\n",
            "    (1): GCNConv(64, 64)\n",
            "  )\n",
            ")\n",
            "Community Locator finish initialization!\n",
            "\n",
            "Training Order Embedding ... \n",
            "***epoch: 0001 | ORDER EMBEDDING train_loss: 303.15408 | cost time 0.84s\n",
            "***epoch: 0002 | ORDER EMBEDDING train_loss: 257.04663 | cost time 0.659s\n",
            "***epoch: 0003 | ORDER EMBEDDING train_loss: 306.61084 | cost time 0.656s\n",
            "***epoch: 0004 | ORDER EMBEDDING train_loss: 293.03113 | cost time 0.884s\n",
            "***epoch: 0005 | ORDER EMBEDDING train_loss: 261.08939 | cost time 1.08s\n",
            "***epoch: 0006 | ORDER EMBEDDING train_loss: 286.88300 | cost time 1.02s\n",
            "***epoch: 0007 | ORDER EMBEDDING train_loss: 236.51346 | cost time 0.987s\n",
            "***epoch: 0008 | ORDER EMBEDDING train_loss: 242.91031 | cost time 0.637s\n",
            "***epoch: 0009 | ORDER EMBEDDING train_loss: 236.53575 | cost time 0.696s\n",
            "***epoch: 0010 | ORDER EMBEDDING train_loss: 208.26244 | cost time 0.648s\n",
            "***epoch: 0011 | ORDER EMBEDDING train_loss: 232.44637 | cost time 0.657s\n",
            "***epoch: 0012 | ORDER EMBEDDING train_loss: 215.11362 | cost time 0.66s\n",
            "***epoch: 0013 | ORDER EMBEDDING train_loss: 191.37244 | cost time 0.661s\n",
            "***epoch: 0014 | ORDER EMBEDDING train_loss: 201.03522 | cost time 0.612s\n",
            "***epoch: 0015 | ORDER EMBEDDING train_loss: 216.37047 | cost time 0.679s\n",
            "***epoch: 0016 | ORDER EMBEDDING train_loss: 173.29025 | cost time 0.649s\n",
            "***epoch: 0017 | ORDER EMBEDDING train_loss: 158.26759 | cost time 0.655s\n",
            "***epoch: 0018 | ORDER EMBEDDING train_loss: 164.42181 | cost time 0.62s\n",
            "***epoch: 0019 | ORDER EMBEDDING train_loss: 177.63791 | cost time 0.664s\n",
            "***epoch: 0020 | ORDER EMBEDDING train_loss: 153.90561 | cost time 0.654s\n",
            "***epoch: 0021 | ORDER EMBEDDING train_loss: 153.05182 | cost time 0.639s\n",
            "***epoch: 0022 | ORDER EMBEDDING train_loss: 145.21747 | cost time 0.65s\n",
            "***epoch: 0023 | ORDER EMBEDDING train_loss: 175.62888 | cost time 0.938s\n",
            "***epoch: 0024 | ORDER EMBEDDING train_loss: 129.38824 | cost time 1.03s\n",
            "***epoch: 0025 | ORDER EMBEDDING train_loss: 145.93777 | cost time 1.04s\n",
            "***epoch: 0026 | ORDER EMBEDDING train_loss: 147.38869 | cost time 1.0s\n",
            "***epoch: 0027 | ORDER EMBEDDING train_loss: 136.10901 | cost time 0.628s\n",
            "***epoch: 0028 | ORDER EMBEDDING train_loss: 118.08105 | cost time 0.644s\n",
            "***epoch: 0029 | ORDER EMBEDDING train_loss: 122.45334 | cost time 0.652s\n",
            "***epoch: 0030 | ORDER EMBEDDING train_loss: 120.06828 | cost time 0.635s\n",
            "Order Embedding Finish Training!\n",
            "\n",
            "***Generate nodes embedding from idx 0 to 4096\n",
            "***Generate nodes embedding from idx 4096 to 8192\n",
            "***Generate nodes embedding from idx 8192 to 11369\n",
            "\n",
            "Start Matching ... \n",
            "[Generate] Pred size 1000, Avg Length 8.3390\n",
            "\n",
            "P, R, F, J AvgAxis0:  [0.28169181 0.7395131  0.39145629 0.25322272]\n",
            "P, R, F, J AvgAxis1:  [0.5949418  0.20919709 0.30052591 0.19578356]\n",
            "AvgF1: 0.3460 AvgJaccard: 0.2245 NMI: 0.0346 Detect percent: 0.6774\n",
            "[Eval-Epoch140] Improve f1 0.0022, improve jaccard 0.0021, improve new_nmi 0.0370\n",
            "[Eval-Epoch180] Improve f1 -0.0052, improve jaccard -0.0029, improve new_nmi 0.0368\n",
            "[Eval-Epoch220] Improve f1 0.0147, improve jaccard 0.0120, improve new_nmi 0.0387\n",
            "[Eval-Epoch240] Improve f1 -0.0036, improve jaccard -0.0025, improve new_nmi 0.0362\n",
            "[Eval-Epoch260] Improve f1 0.0208, improve jaccard 0.0157, improve new_nmi 0.0485\n",
            "[Eval-Epoch280] Improve f1 0.0137, improve jaccard 0.0108, improve new_nmi 0.0462\n",
            "[Eval-Epoch300] Improve f1 0.0253, improve jaccard 0.0189, improve new_nmi 0.0518\n",
            "[Eval-Epoch320] Improve f1 0.0202, improve jaccard 0.0157, improve new_nmi 0.0457\n",
            "[Eval-Epoch340] Improve f1 0.0399, improve jaccard 0.0296, improve new_nmi 0.0495\n",
            "[Eval-Epoch360] Improve f1 0.0214, improve jaccard 0.0152, improve new_nmi 0.0461\n",
            "[Eval-Epoch380] Improve f1 0.0386, improve jaccard 0.0296, improve new_nmi 0.0497\n",
            "[Eval-Epoch400] Improve f1 0.0102, improve jaccard 0.0078, improve new_nmi 0.0383\n",
            "[Eval-Epoch420] Improve f1 0.0167, improve jaccard 0.0141, improve new_nmi 0.0564\n",
            "[Eval-Epoch440] Improve f1 0.0238, improve jaccard 0.0179, improve new_nmi 0.0489\n",
            "[Eval-Epoch460] Improve f1 0.0153, improve jaccard 0.0114, improve new_nmi 0.0488\n",
            "[Eval-Epoch480] Improve f1 0.0161, improve jaccard 0.0121, improve new_nmi 0.0361\n",
            "[Eval-Epoch500] Improve f1 0.0099, improve jaccard 0.0076, improve new_nmi 0.0466\n",
            "[Eval-Epoch520] Improve f1 0.0211, improve jaccard 0.0172, improve new_nmi 0.0384\n",
            "[Eval-Epoch540] Improve f1 0.0299, improve jaccard 0.0232, improve new_nmi 0.0552\n",
            "[Eval-Epoch560] Improve f1 0.0093, improve jaccard 0.0063, improve new_nmi 0.0466\n",
            "[Eval-Epoch580] Improve f1 0.0230, improve jaccard 0.0164, improve new_nmi 0.0461\n",
            "[Eval-Epoch600] Improve f1 0.0298, improve jaccard 0.0217, improve new_nmi 0.0488\n",
            "[Eval-Epoch620] Improve f1 0.0203, improve jaccard 0.0152, improve new_nmi 0.0502\n",
            "[Eval-Epoch640] Improve f1 0.0297, improve jaccard 0.0218, improve new_nmi 0.0463\n",
            "[Eval-Epoch660] Improve f1 0.0165, improve jaccard 0.0122, improve new_nmi 0.0491\n",
            "[Eval-Epoch680] Improve f1 0.0292, improve jaccard 0.0213, improve new_nmi 0.0462\n",
            "[Eval-Epoch700] Improve f1 0.0156, improve jaccard 0.0122, improve new_nmi 0.0489\n",
            "[Eval-Epoch720] Improve f1 0.0153, improve jaccard 0.0111, improve new_nmi 0.0467\n",
            "[Eval-Epoch740] Improve f1 0.0106, improve jaccard 0.0079, improve new_nmi 0.0465\n",
            "[Eval-Epoch760] Improve f1 0.0123, improve jaccard 0.0092, improve new_nmi 0.0464\n",
            "[Eval-Epoch780] Improve f1 0.0331, improve jaccard 0.0241, improve new_nmi 0.0461\n",
            "[Eval-Epoch800] Improve f1 0.0192, improve jaccard 0.0139, improve new_nmi 0.0464\n",
            "[Eval-Epoch820] Improve f1 0.0315, improve jaccard 0.0232, improve new_nmi 0.0462\n",
            "[Eval-Epoch840] Improve f1 0.0247, improve jaccard 0.0184, improve new_nmi 0.0362\n",
            "[Eval-Epoch860] Improve f1 0.0282, improve jaccard 0.0205, improve new_nmi 0.0462\n",
            "[Eval-Epoch880] Improve f1 0.0246, improve jaccard 0.0179, improve new_nmi 0.0465\n",
            "[Eval-Epoch900] Improve f1 0.0271, improve jaccard 0.0205, improve new_nmi 0.0462\n",
            "[Eval-Epoch920] Improve f1 0.0179, improve jaccard 0.0129, improve new_nmi 0.0466\n",
            "[Eval-Epoch940] Improve f1 0.0101, improve jaccard 0.0078, improve new_nmi 0.0465\n",
            "[Eval-Epoch960] Improve f1 0.0246, improve jaccard 0.0179, improve new_nmi 0.0465\n",
            "[Eval-Epoch980] Improve f1 0.0177, improve jaccard 0.0130, improve new_nmi 0.0466\n",
            "[Eval-Epoch1000] Improve f1 0.0177, improve jaccard 0.0130, improve new_nmi 0.0466\n",
            "Load net from ckpts/orkut/20231211-215344/commr_eval_best.pt at Epoch779\n",
            "[Rewrite] Pred size 1000, Avg Length 7.4690\n",
            "P, R, F, J AvgAxis0:  [0.30387074 0.71229405 0.40400231 0.26583527]\n",
            "P, R, F, J AvgAxis1:  [0.58140476 0.23132131 0.31664804 0.21099947]\n",
            "AvgF1: 0.3603 AvgJaccard: 0.2384 NMI: 0.0577 Detect percent: 0.6579\n",
            "## Finishing Time: 2023-12-11 21:56:42\n",
            "= = = = = = = = = = = = = = = = = = = = \n"
          ]
        }
      ],
      "source": [
        "!python main.py --dataset=orkut"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}